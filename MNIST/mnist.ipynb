{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device configuration: use GPU if available, otherwise fallback to CPU\n",
    "# Konfigurasi perangkat: gunakan GPU jika tersedia, jika tidak gunakan CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  # Print device being used (for debugging) / Cetak perangkat yang digunakan (untuk debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters: set training parameters\n",
    "# Hyperparameter: atur parameter pelatihan\n",
    "num_epochs = 100  # Number of complete passes through the dataset / Jumlah siklus lengkap melalui dataset\n",
    "num_classes = 10  # Number of output classes (digits 0-9) / Jumlah kelas keluaran (digit 0-9)\n",
    "batch_size = 256  # Number of samples processed before updating model parameters / Jumlah sampel yang diproses sebelum memperbarui parameter model\n",
    "learning_rate = 0.001  # Step size used by the optimizer to adjust the model's weights / Ukuran langkah yang digunakan oleh optimizer untuk menyesuaikan bobot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation with transforms: apply transformations to the training data to increase variety\n",
    "# Augmentasi data dengan transformasi: terapkan transformasi pada data pelatihan untuk meningkatkan variasi\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees / Putar gambar secara acak hingga 10 derajat\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Randomly translate the image by up to 10% in both directions / Pindahkan gambar secara acak hingga 10% di kedua arah\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor / Konversi gambar menjadi tensor PyTorch\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using the mean and std for MNIST dataset / Normalisasi menggunakan rata-rata dan deviasi standar untuk dataset MNIST\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:16<00:00, 603305.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 113629.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:07<00:00, 217323.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 719512.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and load the MNIST dataset (train and test sets)\n",
    "# Unduh dan muat dataset MNIST (set pelatihan dan pengujian)\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,  # True = load the training set / True = muat set pelatihan\n",
    "                                           transform=transform,  # Apply data augmentations / Terapkan augmentasi data\n",
    "                                           download=True)  # Download the dataset if not available locally / Unduh dataset jika tidak tersedia secara lokal\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,  # False = load the test set / False = muat set pengujian\n",
    "                                          transform=transforms.Compose([\n",
    "                                              transforms.ToTensor(),  # Convert to PyTorch tensor (no augmentation for test set) / Konversi ke tensor PyTorch (tidak ada augmentasi untuk set pengujian)\n",
    "                                              transforms.Normalize((0.1307,), (0.3081,))  # Normalize using the same values / Normalisasi menggunakan nilai yang sama\n",
    "                                          ]))\n",
    "\n",
    "# Data loader: load datasets in batches for training and testing\n",
    "# Pemuat data: muat dataset dalam batch untuk pelatihan dan pengujian\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,  # Batch size for training / Ukuran batch untuk pelatihan\n",
    "                                           shuffle=True)  # Shuffle training data to ensure random batches / Acak data pelatihan untuk memastikan batch yang acak\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,  # Batch size for testing / Ukuran batch untuk pengujian\n",
    "                                          shuffle=False)  # No need to shuffle test data / Tidak perlu mengacak data pengujian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Convolutional Neural Network (CNN) architecture for image classification\n",
    "# Arsitektur Jaringan Saraf Konvolusional (CNN) yang ditingkatkan untuk klasifikasi gambar\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # First convolutional block:\n",
    "        # Input: 28x28x1 (grayscale), Output: 14x14x32\n",
    "        # Blok konvolusi pertama:\n",
    "        # Masukan: 28x28x1 (grayscale), Keluaran: 14x14x32\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),  # Convolution: 32 filters of size 5x5, padding 2 / Konvolusi: 32 filter berukuran 5x5, padding 2\n",
    "            nn.BatchNorm2d(32),  # Batch normalization: normalize activations to improve convergence / Normalisasi batch: menormalkan aktivasi untuk meningkatkan konvergensi\n",
    "            nn.ReLU(),  # Activation function: introduces non-linearity / Fungsi aktivasi: memperkenalkan non-linearitas\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  # Max pooling: reduce feature map size by 2x (downsampling) / Max pooling: mengurangi ukuran peta fitur menjadi 2x (downsampling)\n",
    "\n",
    "        # Second convolutional block:\n",
    "        # Input: 14x14x32, Output: 7x7x64\n",
    "        # Blok konvolusi kedua:\n",
    "        # Masukan: 14x14x32, Keluaran: 7x7x64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  # Convolution: 64 filters of size 5x5 / Konvolusi: 64 filter berukuran 5x5\n",
    "            nn.BatchNorm2d(64),  # Batch normalization / Normalisasi batch\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  # Max pooling\n",
    "\n",
    "        # Third convolutional block (added for improved accuracy):\n",
    "        # Input: 7x7x64, Output: 3x3x128\n",
    "        # Blok konvolusi ketiga (ditambahkan untuk meningkatkan akurasi):\n",
    "        # Masukan: 7x7x64, Keluaran: 3x3x128\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Convolution: 128 filters of size 3x3 / Konvolusi: 128 filter berukuran 3x3\n",
    "            nn.BatchNorm2d(128),  # Batch normalization / Normalisasi batch\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  # Max pooling\n",
    "\n",
    "        # Dropout layer: randomly set some weights to zero during training to prevent overfitting\n",
    "        # Lapisan dropout: secara acak mengatur beberapa bobot menjadi nol selama pelatihan untuk mencegah overfitting\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully connected layers (classifier):\n",
    "        # Lapisan sepenuhnya terhubung (klasifier):\n",
    "        self.fc1 = nn.Linear(3*3*128, 512)  # Input is flattened from 3x3x128, output is 512 / Masukan diratakan dari 3x3x128, keluaran adalah 512\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Final output layer, 10 output classes / Lapisan keluaran akhir, 10 kelas keluaran\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)  # Pass input through layer 1 / Lewatkan masukan melalui lapisan 1\n",
    "        out = self.layer2(out)  # Pass through layer 2 / Lewatkan melalui lapisan 2\n",
    "        out = self.layer3(out)  # Pass through additional layer 3 / Lewatkan melalui lapisan tambahan 3\n",
    "        out = out.reshape(out.size(0), -1)  # Flatten the output from conv layers / Ratakan keluaran dari lapisan konvolusi\n",
    "        out = self.dropout(out)  # Apply dropout / Terapkan dropout\n",
    "        out = self.fc1(out)  # Pass through first fully connected layer / Lewatkan melalui lapisan sepenuhnya terhubung pertama\n",
    "        out = self.fc2(out)  # Output from final fully connected layer / Keluaran dari lapisan sepenuhnya terhubung terakhir\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and move it to the GPU/CPU\n",
    "# Buat model dan pindahkan ke GPU/CPU\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss function: Cross entropy loss, suitable for multi-class classification problems\n",
    "# Fungsi kerugian: Kerugian entropi silang, cocok untuk masalah klasifikasi multi-kelas\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: AdamW optimizer with weight decay to prevent overfitting\n",
    "# Optimizer: Optimizer AdamW dengan pengurangan bobot untuk mencegah overfitting\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler: Reduce learning rate by a factor of 0.7 every 3 epochs\n",
    "# Penjadwal laju pembelajaran: Kurangi laju pembelajaran dengan faktor 0,7 setiap 3 epoch\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Avg Loss: 0.3541\n",
      "Epoch [2/100], Avg Loss: 0.0975\n",
      "Epoch [3/100], Avg Loss: 0.0768\n",
      "Epoch [4/100], Avg Loss: 0.0586\n",
      "Epoch [5/100], Avg Loss: 0.0557\n",
      "Epoch [6/100], Avg Loss: 0.0481\n",
      "Epoch [7/100], Avg Loss: 0.0468\n",
      "Epoch [8/100], Avg Loss: 0.0409\n",
      "Epoch [9/100], Avg Loss: 0.0411\n",
      "Epoch [10/100], Avg Loss: 0.0341\n",
      "Epoch [11/100], Avg Loss: 0.0343\n",
      "Epoch [12/100], Avg Loss: 0.0327\n",
      "Epoch [13/100], Avg Loss: 0.0300\n",
      "Epoch [14/100], Avg Loss: 0.0293\n",
      "Epoch [15/100], Avg Loss: 0.0281\n",
      "Epoch [16/100], Avg Loss: 0.0265\n",
      "Epoch [17/100], Avg Loss: 0.0244\n",
      "Epoch [18/100], Avg Loss: 0.0257\n",
      "Epoch [19/100], Avg Loss: 0.0241\n",
      "Epoch [20/100], Avg Loss: 0.0227\n",
      "Epoch [21/100], Avg Loss: 0.0226\n",
      "Epoch [22/100], Avg Loss: 0.0212\n",
      "Epoch [23/100], Avg Loss: 0.0214\n",
      "Epoch [24/100], Avg Loss: 0.0206\n",
      "Epoch [25/100], Avg Loss: 0.0212\n",
      "Epoch [26/100], Avg Loss: 0.0206\n",
      "Epoch [27/100], Avg Loss: 0.0192\n",
      "Epoch [28/100], Avg Loss: 0.0191\n",
      "Epoch [29/100], Avg Loss: 0.0195\n",
      "Epoch [30/100], Avg Loss: 0.0186\n",
      "Epoch [31/100], Avg Loss: 0.0191\n",
      "Epoch [32/100], Avg Loss: 0.0186\n",
      "Epoch [33/100], Avg Loss: 0.0185\n",
      "Epoch [34/100], Avg Loss: 0.0186\n",
      "Epoch [35/100], Avg Loss: 0.0172\n",
      "Epoch [36/100], Avg Loss: 0.0176\n",
      "Epoch [37/100], Avg Loss: 0.0177\n",
      "Epoch [38/100], Avg Loss: 0.0159\n",
      "Epoch [39/100], Avg Loss: 0.0161\n",
      "Epoch [40/100], Avg Loss: 0.0170\n",
      "Epoch [41/100], Avg Loss: 0.0165\n",
      "Epoch [42/100], Avg Loss: 0.0178\n",
      "Epoch [43/100], Avg Loss: 0.0172\n",
      "Epoch [44/100], Avg Loss: 0.0156\n",
      "Epoch [45/100], Avg Loss: 0.0179\n",
      "Epoch [46/100], Avg Loss: 0.0164\n",
      "Epoch [47/100], Avg Loss: 0.0171\n",
      "Epoch [48/100], Avg Loss: 0.0161\n",
      "Epoch [49/100], Avg Loss: 0.0166\n",
      "Epoch [50/100], Avg Loss: 0.0166\n",
      "Epoch [51/100], Avg Loss: 0.0167\n",
      "Epoch [52/100], Avg Loss: 0.0169\n",
      "Epoch [53/100], Avg Loss: 0.0163\n",
      "Epoch [54/100], Avg Loss: 0.0174\n",
      "Epoch [55/100], Avg Loss: 0.0162\n",
      "Epoch [56/100], Avg Loss: 0.0166\n",
      "Epoch [57/100], Avg Loss: 0.0175\n",
      "Epoch [58/100], Avg Loss: 0.0162\n",
      "Epoch [59/100], Avg Loss: 0.0163\n",
      "Epoch [60/100], Avg Loss: 0.0168\n",
      "Epoch [61/100], Avg Loss: 0.0168\n",
      "Epoch [62/100], Avg Loss: 0.0165\n",
      "Epoch [63/100], Avg Loss: 0.0170\n",
      "Epoch [64/100], Avg Loss: 0.0169\n",
      "Epoch [65/100], Avg Loss: 0.0165\n",
      "Epoch [66/100], Avg Loss: 0.0164\n",
      "Epoch [67/100], Avg Loss: 0.0162\n",
      "Epoch [68/100], Avg Loss: 0.0166\n",
      "Epoch [69/100], Avg Loss: 0.0155\n",
      "Epoch [70/100], Avg Loss: 0.0167\n",
      "Epoch [71/100], Avg Loss: 0.0163\n",
      "Epoch [72/100], Avg Loss: 0.0164\n",
      "Epoch [73/100], Avg Loss: 0.0167\n",
      "Epoch [74/100], Avg Loss: 0.0170\n",
      "Epoch [75/100], Avg Loss: 0.0165\n",
      "Epoch [76/100], Avg Loss: 0.0146\n",
      "Epoch [77/100], Avg Loss: 0.0171\n",
      "Epoch [78/100], Avg Loss: 0.0153\n",
      "Epoch [79/100], Avg Loss: 0.0169\n",
      "Epoch [80/100], Avg Loss: 0.0164\n",
      "Epoch [81/100], Avg Loss: 0.0159\n",
      "Epoch [82/100], Avg Loss: 0.0171\n",
      "Epoch [83/100], Avg Loss: 0.0163\n",
      "Epoch [84/100], Avg Loss: 0.0173\n",
      "Epoch [85/100], Avg Loss: 0.0171\n",
      "Epoch [86/100], Avg Loss: 0.0161\n",
      "Epoch [87/100], Avg Loss: 0.0162\n",
      "Epoch [88/100], Avg Loss: 0.0160\n",
      "Epoch [89/100], Avg Loss: 0.0172\n",
      "Epoch [90/100], Avg Loss: 0.0171\n",
      "Epoch [91/100], Avg Loss: 0.0168\n",
      "Epoch [92/100], Avg Loss: 0.0162\n",
      "Epoch [93/100], Avg Loss: 0.0165\n",
      "Epoch [94/100], Avg Loss: 0.0166\n",
      "Epoch [95/100], Avg Loss: 0.0157\n",
      "Epoch [96/100], Avg Loss: 0.0164\n",
      "Epoch [97/100], Avg Loss: 0.0164\n",
      "Epoch [98/100], Avg Loss: 0.0162\n",
      "Epoch [99/100], Avg Loss: 0.0166\n",
      "Epoch [100/100], Avg Loss: 0.0160\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Latih model\n",
    "total_step = len(train_loader)  # Total number of steps (batches) per epoch / Total jumlah langkah (batch) per epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode / Setel model ke mode pelatihan\n",
    "    \n",
    "    cumulative_loss = 0.0  # Initialize cumulative loss for the epoch\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)  # Move images to GPU/CPU / Pindahkan gambar ke GPU/CPU\n",
    "        labels = labels.to(device)  # Move labels to GPU/CPU / Pindahkan label ke GPU/CPU\n",
    "\n",
    "        # Forward pass: compute model predictions\n",
    "        # Proses maju: hitung prediksi model\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)  # Compute loss between predictions and true labels / Hitung kerugian antara prediksi dan label yang benar\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        # Proses mundur: hitung gradien\n",
    "        optimizer.zero_grad()  # Clear old gradients / Hapus gradien lama\n",
    "        loss.backward()  # Backpropagation: compute gradients / Backpropagation: hitung gradien\n",
    "        optimizer.step()  # Update model parameters / Perbarui parameter model\n",
    "        \n",
    "        cumulative_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = cumulative_loss / total_step\n",
    "\n",
    "    # Log the epoch number and average loss\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    scheduler.step()  # Step the scheduler at the end of each epoch to adjust learning rate / Langkah penjadwal di akhir setiap epoch untuk menyesuaikan laju pembelajaran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.58 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# Uji model\n",
    "model.eval()  # Set model to evaluation mode (disable dropout, use running averages for batchnorm) / Setel model ke mode evaluasi (nonaktifkan dropout, gunakan rata-rata berjalan untuk batchnorm)\n",
    "with torch.no_grad():  # No need to compute gradients for testing / Tidak perlu menghitung gradien untuk pengujian\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)  # Get model predictions / Dapatkan prediksi model\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Select class with the highest score / Pilih kelas dengan skor tertinggi\n",
    "        total += labels.size(0)  # Increment total number of images / Tambahkan total jumlah gambar\n",
    "        correct += (predicted == labels).sum().item()  # Count correct predictions / Hitung prediksi yang benar\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "    # Print test accuracy / Cetak akurasi pengujian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "# Simpan model checkpoint\n",
    "torch.save(model.state_dict(), 'model_improved.ckpt')  # Save the model's state (weights) / Simpan status model (bobot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
