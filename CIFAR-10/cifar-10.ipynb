{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration: set the device to GPU if available, otherwise use CPU\n",
    "# Konfigurasi perangkat: atur perangkat ke GPU jika tersedia, jika tidak gunakan CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters: defining the main parameters for training the model\n",
    "# Hyperparameter: mendefinisikan parameter utama untuk melatih model\n",
    "num_epochs = 100  # Number of epochs for training; CIFAR-10 often requires more epochs to converge\n",
    "# Jumlah epoch untuk pelatihan; CIFAR-10 sering membutuhkan lebih banyak epoch untuk konvergensi\n",
    "num_classes = 10  # Total number of classes in the CIFAR-10 dataset (0-9)\n",
    "batch_size = 256  # Increased batch size for better training efficiency\n",
    "# Ukuran batch yang ditingkatkan untuk efisiensi pelatihan yang lebih baik\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "# Laju pembelajaran untuk optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training data\n",
    "# Augmentasi data dan normalisasi untuk data pelatihan\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image with padding to increase variety\n",
    "    # Memotong gambar secara acak dengan padding untuk meningkatkan variasi\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    # Membalik gambar secara acak secara horizontal\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    # Mengonversi gambar ke tensor PyTorch\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize with CIFAR-10 mean and std\n",
    "    # Normalisasi dengan rata-rata dan deviasi standar CIFAR-10\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation for test set (no augmentation, only normalization)\n",
    "# Transformasi data untuk set pengujian (tanpa augmentasi, hanya normalisasi)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    # Mengonversi gambar ke tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))  # Normalize using the same mean and std\n",
    "    # Normalisasi menggunakan rata-rata dan deviasi standar yang sama\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading CIFAR-10 dataset\n",
    "# Memuat dataset CIFAR-10\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                             train=True,  # Load training data\n",
    "                                             transform=transform_train,  # Apply training transformations\n",
    "                                             download=True)  # Download the dataset if not available locally\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
    "                                            train=False,  # Load testing data\n",
    "                                            transform=transform_test)  # Apply testing transformations\n",
    "\n",
    "# Data loader: Load datasets in batches for training and testing\n",
    "# Pemuat data: Memuat dataset dalam batch untuk pelatihan dan pengujian\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,  # Number of samples per batch\n",
    "                                           shuffle=True)  # Shuffle the training data for randomness\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,  # Number of samples per batch\n",
    "                                          shuffle=False)  # No need to shuffle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional Neural Network (CNN) for CIFAR-10 classification\n",
    "# Jaringan Saraf Konvolusional (CNN) dalam untuk klasifikasi CIFAR-10\n",
    "class CIFAR10Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CIFAR10Net, self).__init__()\n",
    "        # Define the first convolutional block\n",
    "        # Mendefinisikan blok konvolusi pertama\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # First convolution layer with 64 filters\n",
    "            # Lapisan konvolusi pertama dengan 64 filter\n",
    "            nn.BatchNorm2d(64),  # Batch normalization for the first layer\n",
    "            # Normalisasi batch untuk lapisan pertama\n",
    "            nn.ReLU(),  # Activation function to introduce non-linearity\n",
    "            # Fungsi aktivasi untuk memperkenalkan non-linearitas\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Second convolution layer with 64 filters\n",
    "            # Lapisan konvolusi kedua dengan 64 filter\n",
    "            nn.BatchNorm2d(64),  # Batch normalization for the second layer\n",
    "            # Normalisasi batch untuk lapisan kedua\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling to reduce spatial dimensions by half\n",
    "            # Max pooling untuk mengurangi dimensi spasial hingga setengah\n",
    "        )\n",
    "        # Define the second convolutional block\n",
    "        # Mendefinisikan blok konvolusi kedua\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # First convolution layer with 128 filters\n",
    "            # Lapisan konvolusi pertama dengan 128 filter\n",
    "            nn.BatchNorm2d(128),  # Batch normalization for the first layer\n",
    "            # Normalisasi batch untuk lapisan pertama\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),  # Second convolution layer with 128 filters\n",
    "            # Lapisan konvolusi kedua dengan 128 filter\n",
    "            nn.BatchNorm2d(128),  # Batch normalization for the second layer\n",
    "            # Normalisasi batch untuk lapisan kedua\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling to reduce spatial dimensions by half\n",
    "            # Max pooling untuk mengurangi dimensi spasial hingga setengah\n",
    "        )\n",
    "        # Define the third convolutional block\n",
    "        # Mendefinisikan blok konvolusi ketiga\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # First convolution layer with 256 filters\n",
    "            # Lapisan konvolusi pertama dengan 256 filter\n",
    "            nn.BatchNorm2d(256),  # Batch normalization for the first layer\n",
    "            # Normalisasi batch untuk lapisan pertama\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),  # Second convolution layer with 256 filters\n",
    "            # Lapisan konvolusi kedua dengan 256 filter\n",
    "            nn.BatchNorm2d(256),  # Batch normalization for the second layer\n",
    "            # Normalisasi batch untuk lapisan kedua\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling to reduce spatial dimensions by half\n",
    "            # Max pooling untuk mengurangi dimensi spasial hingga setengah\n",
    "        )\n",
    "        # Define the fourth convolutional block\n",
    "        # Mendefinisikan blok konvolusi keempat\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),  # First convolution layer with 512 filters\n",
    "            # Lapisan konvolusi pertama dengan 512 filter\n",
    "            nn.BatchNorm2d(512),  # Batch normalization for the first layer\n",
    "            # Normalisasi batch untuk lapisan pertama\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),  # Second convolution layer with 512 filters\n",
    "            # Lapisan konvolusi kedua dengan 512 filter\n",
    "            nn.BatchNorm2d(512),  # Batch normalization for the second layer\n",
    "            # Normalisasi batch untuk lapisan kedua\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling to reduce spatial dimensions by half\n",
    "            # Max pooling untuk mengurangi dimensi spasial hingga setengah\n",
    "        )\n",
    "        # Dropout layer to prevent overfitting\n",
    "        # Lapisan dropout untuk mencegah overfitting\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n",
    "        # Dropout dengan probabilitas 50%\n",
    "        # Fully connected layers for classification\n",
    "        # Lapisan sepenuhnya terhubung untuk klasifikasi\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)  # Input is flattened from 512*2*2 features\n",
    "        # Masukan diratakan dari 512*2*2 fitur\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Final output layer producing num_classes outputs\n",
    "        # Lapisan keluaran akhir menghasilkan keluaran jumlah kelas\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)  # Forward pass through layer 1\n",
    "        # Proses maju melalui lapisan 1\n",
    "        out = self.layer2(out)  # Forward pass through layer 2\n",
    "        # Proses maju melalui lapisan 2\n",
    "        out = self.layer3(out)  # Forward pass through layer 3\n",
    "        # Proses maju melalui lapisan 3\n",
    "        out = self.layer4(out)  # Forward pass through layer 4\n",
    "        # Proses maju melalui lapisan 4\n",
    "        out = out.view(out.size(0), -1)  # Flatten the tensor for fully connected layers\n",
    "        # Ratakan tensor untuk lapisan sepenuhnya terhubung\n",
    "        out = self.dropout(out)  # Apply dropout to prevent overfitting\n",
    "        # Terapkan dropout untuk mencegah overfitting\n",
    "        out = self.fc1(out)  # First fully connected layer\n",
    "        # Lapisan sepenuhnya terhubung pertama\n",
    "        out = self.fc2(out)  # Final output layer\n",
    "        # Lapisan keluaran akhir\n",
    "        return out  # Return the output predictions\n",
    "        # Mengembalikan prediksi keluaran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the configured device\n",
    "# Inisialisasi model dan pindahkan ke perangkat yang dikonfigurasi\n",
    "model = CIFAR10Net(num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer definition\n",
    "# Definisi fungsi kerugian dan optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "# Kerugian cross-entropy untuk klasifikasi multi-kelas\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)  # AdamW optimizer\n",
    "# Optimizer AdamW\n",
    "\n",
    "# Learning rate scheduler for adjusting learning rate during training\n",
    "# Penjadwal laju pembelajaran untuk menyesuaikan laju pembelajaran selama pelatihan\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)  # Cosine annealing for smoother convergence\n",
    "# Pengurangan cosinus untuk konvergensi yang lebih halus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Avg Loss: 2.1724\n",
      "Epoch [2/100], Avg Loss: 1.4898\n",
      "Epoch [3/100], Avg Loss: 1.0900\n",
      "Epoch [4/100], Avg Loss: 0.8539\n",
      "Epoch [5/100], Avg Loss: 0.7054\n",
      "Epoch [6/100], Avg Loss: 0.6210\n",
      "Epoch [7/100], Avg Loss: 0.5496\n",
      "Epoch [8/100], Avg Loss: 0.5000\n",
      "Epoch [9/100], Avg Loss: 0.4539\n",
      "Epoch [10/100], Avg Loss: 0.4154\n",
      "Epoch [11/100], Avg Loss: 0.3820\n",
      "Epoch [12/100], Avg Loss: 0.3506\n",
      "Epoch [13/100], Avg Loss: 0.3395\n",
      "Epoch [14/100], Avg Loss: 0.3041\n",
      "Epoch [15/100], Avg Loss: 0.2834\n",
      "Epoch [16/100], Avg Loss: 0.2712\n",
      "Epoch [17/100], Avg Loss: 0.2470\n",
      "Epoch [18/100], Avg Loss: 0.2283\n",
      "Epoch [19/100], Avg Loss: 0.2109\n",
      "Epoch [20/100], Avg Loss: 0.1945\n",
      "Epoch [21/100], Avg Loss: 0.1820\n",
      "Epoch [22/100], Avg Loss: 0.1676\n",
      "Epoch [23/100], Avg Loss: 0.1565\n",
      "Epoch [24/100], Avg Loss: 0.1405\n",
      "Epoch [25/100], Avg Loss: 0.1302\n",
      "Epoch [26/100], Avg Loss: 0.1211\n",
      "Epoch [27/100], Avg Loss: 0.1109\n",
      "Epoch [28/100], Avg Loss: 0.1009\n",
      "Epoch [29/100], Avg Loss: 0.0892\n",
      "Epoch [30/100], Avg Loss: 0.0840\n",
      "Epoch [31/100], Avg Loss: 0.0757\n",
      "Epoch [32/100], Avg Loss: 0.0636\n",
      "Epoch [33/100], Avg Loss: 0.0551\n",
      "Epoch [34/100], Avg Loss: 0.0527\n",
      "Epoch [35/100], Avg Loss: 0.0464\n",
      "Epoch [36/100], Avg Loss: 0.0420\n",
      "Epoch [37/100], Avg Loss: 0.0350\n",
      "Epoch [38/100], Avg Loss: 0.0289\n",
      "Epoch [39/100], Avg Loss: 0.0282\n",
      "Epoch [40/100], Avg Loss: 0.0256\n",
      "Epoch [41/100], Avg Loss: 0.0223\n",
      "Epoch [42/100], Avg Loss: 0.0188\n",
      "Epoch [43/100], Avg Loss: 0.0165\n",
      "Epoch [44/100], Avg Loss: 0.0166\n",
      "Epoch [45/100], Avg Loss: 0.0138\n",
      "Epoch [46/100], Avg Loss: 0.0134\n",
      "Epoch [47/100], Avg Loss: 0.0118\n",
      "Epoch [48/100], Avg Loss: 0.0114\n",
      "Epoch [49/100], Avg Loss: 0.0112\n",
      "Epoch [50/100], Avg Loss: 0.0107\n",
      "Epoch [51/100], Avg Loss: 0.0112\n",
      "Epoch [52/100], Avg Loss: 0.0106\n",
      "Epoch [53/100], Avg Loss: 0.0106\n",
      "Epoch [54/100], Avg Loss: 0.0109\n",
      "Epoch [55/100], Avg Loss: 0.0109\n",
      "Epoch [56/100], Avg Loss: 0.0122\n",
      "Epoch [57/100], Avg Loss: 0.0125\n",
      "Epoch [58/100], Avg Loss: 0.0140\n",
      "Epoch [59/100], Avg Loss: 0.0135\n",
      "Epoch [60/100], Avg Loss: 0.0175\n",
      "Epoch [61/100], Avg Loss: 0.0162\n",
      "Epoch [62/100], Avg Loss: 0.0192\n",
      "Epoch [63/100], Avg Loss: 0.0233\n",
      "Epoch [64/100], Avg Loss: 0.0247\n",
      "Epoch [65/100], Avg Loss: 0.0288\n",
      "Epoch [66/100], Avg Loss: 0.0310\n",
      "Epoch [67/100], Avg Loss: 0.0358\n",
      "Epoch [68/100], Avg Loss: 0.0359\n",
      "Epoch [69/100], Avg Loss: 0.0395\n",
      "Epoch [70/100], Avg Loss: 0.0441\n",
      "Epoch [71/100], Avg Loss: 0.0484\n",
      "Epoch [72/100], Avg Loss: 0.0509\n",
      "Epoch [73/100], Avg Loss: 0.0515\n",
      "Epoch [74/100], Avg Loss: 0.0567\n",
      "Epoch [75/100], Avg Loss: 0.0559\n",
      "Epoch [76/100], Avg Loss: 0.0634\n",
      "Epoch [77/100], Avg Loss: 0.0664\n",
      "Epoch [78/100], Avg Loss: 0.0652\n",
      "Epoch [79/100], Avg Loss: 0.0646\n",
      "Epoch [80/100], Avg Loss: 0.0735\n",
      "Epoch [81/100], Avg Loss: 0.0756\n",
      "Epoch [82/100], Avg Loss: 0.0786\n",
      "Epoch [83/100], Avg Loss: 0.0753\n",
      "Epoch [84/100], Avg Loss: 0.0767\n",
      "Epoch [85/100], Avg Loss: 0.0759\n",
      "Epoch [86/100], Avg Loss: 0.0835\n",
      "Epoch [87/100], Avg Loss: 0.0842\n",
      "Epoch [88/100], Avg Loss: 0.0826\n",
      "Epoch [89/100], Avg Loss: 0.0789\n",
      "Epoch [90/100], Avg Loss: 0.0770\n",
      "Epoch [91/100], Avg Loss: 0.0801\n",
      "Epoch [92/100], Avg Loss: 0.0747\n",
      "Epoch [93/100], Avg Loss: 0.0781\n",
      "Epoch [94/100], Avg Loss: 0.0803\n",
      "Epoch [95/100], Avg Loss: 0.0820\n",
      "Epoch [96/100], Avg Loss: 0.0748\n",
      "Epoch [97/100], Avg Loss: 0.0752\n",
      "Epoch [98/100], Avg Loss: 0.0715\n",
      "Epoch [99/100], Avg Loss: 0.0692\n",
      "Epoch [100/100], Avg Loss: 0.0710\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "# Melatih model\n",
    "total_step = len(train_loader)  # Total number of steps (batches) per epoch\n",
    "# Jumlah total langkah (batch) per epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    cumulative_loss = 0.0  # Initialize cumulative loss for the epoch\n",
    "    \n",
    "    # Atur model ke mode pelatihan\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)  # Move images to the configured device (GPU or CPU)\n",
    "        # Pindahkan gambar ke perangkat yang dikonfigurasi (GPU atau CPU)\n",
    "        labels = labels.to(device)  # Move labels to the configured device\n",
    "        # Pindahkan label ke perangkat yang dikonfigurasi\n",
    "        \n",
    "        # Forward pass: compute model predictions\n",
    "        # Proses maju: hitung prediksi model\n",
    "        outputs = model(images)  # Get model predictions for the input images\n",
    "        # Dapatkan prediksi model untuk gambar masukan\n",
    "        loss = criterion(outputs, labels)  # Compute the loss using the predicted and actual labels\n",
    "        # Hitung kerugian menggunakan label yang diprediksi dan yang sebenarnya\n",
    "        \n",
    "        # Backward pass: compute gradients and update weights\n",
    "        # Proses mundur: hitung gradien dan perbarui bobot\n",
    "        optimizer.zero_grad()  # Clear old gradients from the optimizer\n",
    "        # Hapus gradien lama dari optimizer\n",
    "        loss.backward()  # Backpropagation: compute gradients based on the loss\n",
    "        # Backpropagation: hitung gradien berdasarkan kerugian\n",
    "        optimizer.step()  # Update model parameters using computed gradients\n",
    "        # Perbarui parameter model menggunakan gradien yang dihitung\n",
    "        \n",
    "        cumulative_loss += loss.item()  # Accumulate the loss\n",
    "        \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = cumulative_loss / total_step\n",
    "\n",
    "    # Log the epoch number and average loss\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Avg Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Step the scheduler at the end of each epoch to adjust learning rate\n",
    "    # Langkah penjadwal di akhir setiap epoch untuk menyesuaikan laju pembelajaran\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 90.21 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset\n",
    "# Uji model pada dataset pengujian\n",
    "model.eval()  # Set model to evaluation mode\n",
    "# Atur model ke mode evaluasi\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    # Tidak perlu menghitung gradien selama evaluasi\n",
    "    correct = 0  # Initialize correct predictions count\n",
    "    # Inisialisasi hitungan prediksi yang benar\n",
    "    total = 0  # Initialize total images count\n",
    "    # Inisialisasi total jumlah gambar\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)  # Move images to the configured device\n",
    "        # Pindahkan gambar ke perangkat yang dikonfigurasi\n",
    "        labels = labels.to(device)  # Move labels to the configured device\n",
    "        # Pindahkan label ke perangkat yang dikonfigurasi\n",
    "        outputs = model(images)  # Get model predictions\n",
    "        # Dapatkan prediksi model\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest predicted score\n",
    "        # Dapatkan kelas dengan skor prediksi tertinggi\n",
    "        total += labels.size(0)  # Increment total number of images\n",
    "        # Tambahkan total jumlah gambar\n",
    "        correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
    "        # Hitung jumlah prediksi yang benar\n",
    "\n",
    "    # Print test accuracy\n",
    "    # Cetak akurasi pengujian\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint after training\n",
    "# Simpan checkpoint model setelah pelatihan\n",
    "torch.save(model.state_dict(), 'cifar10_model.ckpt')  # Save the model's state (weights) to a file\n",
    "# Simpan status model (bobot) ke file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
